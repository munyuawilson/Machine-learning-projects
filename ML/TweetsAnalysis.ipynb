{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uadOmTZNHkKw"
      },
      "source": [
        "**Sentiment Analysis**\n",
        "\n",
        "This is a sentiment analysis of tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asFYwivdIbD-"
      },
      "source": [
        "**Loading the Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5wqyVaoIaDg",
        "outputId": "e246633c-b0aa-4bad-d030-8a343b05ac6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.8/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.8/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%pip install textblob\n",
        "import nltk\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4-iWcXAIM3i"
      },
      "source": [
        "**Loading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAzyC3wbIC_v"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv('/content/drive/MyDrive/Twitter_Data.csv')\n",
        "#if implementing it offline use\n",
        "#data=pd.read_csv(\"Twitter_Data.csv\")\n",
        "#remove the category column so as to do a fresh analysis\n",
        "data=data.clean_text\n",
        "\n",
        "data=pd.DataFrame(data)\n",
        "data = data.astype(str)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmVlrlGUPM8j"
      },
      "source": [
        "**Remove stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hioNB5vAPR8i",
        "outputId": "5f5d6955-2c6c-406b-cef8-49edd60c3f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "#Filtering stop words\n",
        "\n",
        "data['tweet_without_stopwords'] = data[\"clean_text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "\n",
        "filtered_data=pd.DataFrame(data['clean_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_AXXcyLMKZS"
      },
      "source": [
        "**Sentiment analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DmhAhITCMPHR",
        "outputId": "d52b4839-cb18-491c-afb2-e2cc01317938"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9fa696cd-05d7-4395-8991-cbaab6c3514a\", \"filtered_data.csv\", 21923888)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "sentiment_list=[]\n",
        "for f in data.tweet_without_stopwords:\n",
        "  sentiment=TextBlob(f).sentiment.polarity\n",
        "  if sentiment > 0:\n",
        "    #positive\n",
        "    sentiment_list.append(1)\n",
        "\n",
        "  elif sentiment < 0:\n",
        "     #negative\n",
        "    sentiment_list.append(-1)\n",
        "\n",
        "  else:\n",
        "    #neutral\n",
        "    sentiment_list.append(0)\n",
        "\n",
        "filtered_data[\"sentiments\"]=sentiment_list\n",
        "filtered_data.to_csv(\"filtered_data.csv\")\n",
        "files.download(\"filtered_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J023FJ3neT-4"
      },
      "source": [
        "**Machine Learning Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEj5WaXweYvV",
        "outputId": "f40c5e16-c17b-435f-cdd8-dee581c0717e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9632470241747454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "X=data.tweet_without_stopwords\n",
        "y=filtered_data.sentiments\n",
        "\n",
        "filtered_train,filtered_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "#use Bag of Words to convert the text data into numerical data\n",
        "vectorizer=CountVectorizer()\n",
        "vectorizer.fit(filtered_train)\n",
        "x_train=vectorizer.transform(filtered_train)\n",
        "x_test=vectorizer.transform(filtered_test)\n",
        "\n",
        "#Training\n",
        "model=LogisticRegression()\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "#Accuracy score\n",
        "score=model.score(x_test,y_test)\n",
        "\n",
        "print(score)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}